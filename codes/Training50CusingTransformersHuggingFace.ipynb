{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# In this repo you can finetune any sequence classification transformer model on your dataset"
      ],
      "metadata": {
        "id": "UybPYeQGoT6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tensorflow\n",
        "! pip install nltk\n",
        "! pip install scikit-learn\n",
        "! pip install transformers\n",
        "! pip install datasets\n",
        "!pip install matplotlib"
      ],
      "metadata": {
        "id": "lf6NQcQecLOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDKrQ024oOTQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import datetime\n",
        "import time\n",
        "import torch\n",
        "import random\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from transformers import AutoModel, AutoTokenizer,BertForSequenceClassification, AdamW, BertConfig,get_linear_schedule_with_warmup\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "\n",
        "def normalizeToken(token):\n",
        "    token = token.strip()\n",
        "    lowercased_token = token.lower().strip()\n",
        "    # print(token)\n",
        "    if token != \" \":\n",
        "        if token.startswith(\"@\"):\n",
        "            return \"@USER\"\n",
        "        elif lowercased_token.startswith(\"http\") or lowercased_token.startswith(\"www\"):\n",
        "            return \"HTTPURL\"\n",
        "        # elif len(token) == 1:\n",
        "        #     return demojize(token)\n",
        "        else:\n",
        "            if token == \"’\":\n",
        "                return \"'\"\n",
        "            elif token == \"…\":\n",
        "                return \"...\"\n",
        "            else:\n",
        "                return token\n",
        "\n",
        "\n",
        "import re\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "def normalizeTweet(tweet):\n",
        "    tweet = str(tweet)  # Convert to string if tweet is a float\n",
        "    tok = TweetTokenizer()\n",
        "    tokens = tok.tokenize(tweet.replace(\"’\", \"'\").replace(\"…\", \"...\"))\n",
        "    normTweet = \" \".join([normalizeToken(token) for token in tokens])\n",
        "\n",
        "    normTweet = normTweet.replace(\"cannot \", \"can not \").replace(\"n't \", \" n't \").replace(\"n 't \", \" n't \").replace(\"ca n't\", \"can't\").replace(\"ai n't\", \"ain't\")\n",
        "    normTweet = normTweet.replace(\"'m \", \" 'm \").replace(\"'re \", \" 're \").replace(\"'s \", \" 's \").replace(\"'ll \",\" 'll \").replace(\"'d \", \" 'd \").replace(\"'ve \", \" 've \")\n",
        "    normTweet = normTweet.replace(\" p . m .\", \"  p.m.\").replace(\" p . m \", \" p.m \").replace(\" a . m .\",\" a.m.\").replace(\" a . m \",\" a.m \")\n",
        "    normTweet = re.sub(r\",([0-9]{2,4}) , ([0-9]{2,4})\", r\",\\1,\\2\", normTweet)\n",
        "    normTweet = re.sub(r\"([0-9]{1,3}) / ([0-9]{2,4})\", r\"\\1/\\2\", normTweet)\n",
        "    normTweet = re.sub(r\"([0-9]{1,3})- ([0-9]{2,4})\", r\"\\1-\\2\", normTweet)\n",
        "    normTweet = normTweet.lower()\n",
        "    return \" \".join(normTweet.split())\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# this method just for splitting\n",
        "def splitting_method(df_, name1 ,name2, test_size = 0.5):\n",
        "  y = pd.DataFrame(df_, columns = [\"label\"])\n",
        "  X = pd.DataFrame(df_, columns = ['sentence'])\n",
        "\n",
        "  X_train, X_test ,y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=True, random_state=105)\n",
        "\n",
        "  df_t = pd.DataFrame(X_train, columns = ['sentence'])\n",
        "  df_yt = pd.DataFrame(y_train, columns = ['label'])\n",
        "\n",
        "  train_data = pd.concat([df_t, df_yt], axis=1)\n",
        "  train_data.to_csv(name1+\".csv\", index = False,)\n",
        "\n",
        "  df_xtest = pd.DataFrame(X_test, columns = ['sentence'])\n",
        "  df_ytest = pd.DataFrame(y_test, columns = ['label'])\n",
        "\n",
        "  test_data = pd.concat([df_xtest, df_ytest], axis=1)\n",
        "  # print(test_data.isnull().sum())\n",
        "  if test_size != 0.5:\n",
        "\n",
        "    test_data = test_data.drop_duplicates('sentence')\n",
        "\n",
        "  test_data.to_csv(name2+\".csv\", index = False,)\n",
        "\n",
        "  # return train_data\n",
        "  return test_data\n"
      ],
      "metadata": {
        "id": "mPjGAV3npfpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n"
      ],
      "metadata": {
        "id": "zd-C6HDupPSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHECKPOINT\n",
        "CHECKPOINT = \"Youssef320/LSTM-finetuned-50label-15epoch\"             #change this to the path of your target hugging face model\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    CHECKPOINT,\n",
        "    num_labels = 50,   #should be changed with another dataset\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        "    ignore_mismatched_sizes=True,\n",
        ")\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "5U3DRVjwpRWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA loader fun\n",
        "def Data_to_dataloader(File_name):\n",
        "\n",
        "  df = pd.read_csv(File_name)\n",
        "\n",
        "  print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "  # print('Number of dev sentences: {:,}\\n'.format(df_dev.shape[0]))\n",
        "  df['sentence']  = df.sentence.apply(normalizeTweet)\n",
        "  df.dropna()\n",
        "  # df_dev['sentence']  = df_dev.sentence.apply(normalizeTweet)\n",
        "  # df_dev.dropna()\n",
        "\n",
        "\n",
        "\n",
        "  # Get the lists of sentences and their labels.\n",
        "  sentences = df.sentence.values\n",
        "  labels = df.label.values\n",
        "  # sentences_dev = df_dev.sentence.values\n",
        "  # labels_dev = df_dev.label.values\n",
        "  tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
        "\n",
        "\n",
        "  input_ids = []\n",
        "  # input_ids_dev = []\n",
        "\n",
        "  for sent in sentences:\n",
        "    encoded_sent = tokenizer.encode(sent)\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "  # for sent_dev in sentences_dev:\n",
        "  #   encoded_sent_dev = tokenizer.encode(sent_dev)\n",
        "  #   input_ids_dev.append(encoded_sent_dev)\n",
        "\n",
        "\n",
        "  MAX_LEN = 64\n",
        "  #MAX_LEN = 128\n",
        "  print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "  print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "  # input_ids_dev = pad_sequences(input_ids_dev, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "  print('\\nDone.')\n",
        "  # Create attention masks\n",
        "\n",
        "\n",
        "  attention_masks = []\n",
        "  # attention_masks_dev = []\n",
        "  for sent in input_ids:\n",
        "      att_mask = [int(token_id > 0) for token_id in sent]\n",
        "      attention_masks.append(att_mask)\n",
        "\n",
        "  # for sent_dev in input_ids_dev:\n",
        "  #     att_mask_dev = [int(token_id > 0) for token_id in sent_dev]\n",
        "  #     attention_masks_dev.append(att_mask_dev)\n",
        "\n",
        "\n",
        "  train_inputs = input_ids\n",
        "  # validation_inputs = input_ids_dev\n",
        "\n",
        "  train_labels = labels\n",
        "  print(\"train_labels: \",set(train_labels))\n",
        "\n",
        "  # validation_labels = labels_dev\n",
        "  # print(\"validation_labels: \",set(validation_labels))\n",
        "\n",
        "  train_masks = attention_masks\n",
        "  # validation_masks = attention_masks_dev\n",
        "\n",
        "  train_inputs = torch.tensor(train_inputs)\n",
        "  # validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "  train_labels = torch.tensor(train_labels)\n",
        "  print(\"train_labels: \",train_labels)\n",
        "\n",
        "  # validation_labels = torch.tensor(validation_labels)\n",
        "  # print(\"validation_labels: \",validation_labels)\n",
        "\n",
        "  train_masks = torch.tensor(train_masks)\n",
        "  # validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "\n",
        "  batch_size = 64\n",
        "  # Create the DataLoader for our training set.\n",
        "  train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "  # Create the DataLoader for our validation set.\n",
        "  # validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "  # validation_sampler = SequentialSampler(validation_data)\n",
        "  # validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "  return train_dataloader"
      ],
      "metadata": {
        "id": "EXFXsAKmpgzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change the paths to your files\n",
        "train_dataloader = Data_to_dataloader('/mnt/azureml/cr/j/ec6c81310066482c9f2fcc2c79063234/exe/wd/TopLabeledTrain50V3.csv')\n",
        "validation_dataloader = Data_to_dataloader('/mnt/azureml/cr/j/ec6c81310066482c9f2fcc2c79063234/exe/wd/TopLabeledTest50V3.csv')"
      ],
      "metadata": {
        "id": "5CiJDSUMpjPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fun(epoch_i, train_dataloader, model):\n",
        "\n",
        "  optimizer = AdamW(model.parameters(),\n",
        "                    lr = 2e-5,\n",
        "                    eps = 1e-8\n",
        "                  )\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                              num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                              num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "  # This training code is based on the `run_glue.py` script here:\n",
        "  # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "  seed_val = 42\n",
        "  random.seed(seed_val)\n",
        "  np.random.seed(seed_val)\n",
        "  torch.manual_seed(seed_val)\n",
        "  torch.cuda.manual_seed_all(seed_val)\n",
        "  # Store the average loss after each epoch so we can plot them.\n",
        "  loss_values = []\n",
        "\n",
        "  # ========================================\n",
        "  #               Training\n",
        "  # ========================================\n",
        "  # Perform one full pass over the training set.\n",
        "  print(\"\")\n",
        "  print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "  print('Training...')\n",
        "  # Measure how long the training epoch takes.\n",
        "  t0 = time.time()\n",
        "  # Reset the total loss for this epoch.\n",
        "  total_loss = 0\n",
        "  # Put the model into training mode. Don't be mislead--the call to\n",
        "  # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "  # `dropout` and `batchnorm` layers behave differently during training\n",
        "  # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "  model.train()\n",
        "\n",
        "  best_loss = float('inf')  # Initialize best loss to infinity\n",
        "\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "      if step % 40 == 0 and not step == 0:\n",
        "          elapsed = format_time(time.time() - t0)\n",
        "          print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "      b_input_ids = batch[0].to(device)\n",
        "      b_input_mask = batch[1].to(device)\n",
        "      b_labels = batch[2].to(device)\n",
        "      # Always clear any previously calculated gradients before performing a\n",
        "      # backward pass. PyTorch doesn't do this automatically because\n",
        "      # accumulating the gradients is \"convenient while training RNNs\".\n",
        "      # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "      model.zero_grad()\n",
        "      # The documentation for this `model` function is here:\n",
        "      # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "      outputs = model(b_input_ids,\n",
        "                  token_type_ids=None,\n",
        "                  attention_mask=b_input_mask,\n",
        "                  labels=b_labels)\n",
        "\n",
        "      loss = outputs[0]\n",
        "      total_loss += loss.item()\n",
        "      #loss = calc_loss(outputs[1],b_labels)\n",
        "      #total_loss += loss\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "\n",
        "  avg_train_loss = total_loss / len(train_dataloader)\n",
        "  losses.append(avg_train_loss)\n",
        "  loss_values.append(avg_train_loss)\n",
        "  print(\"\")\n",
        "  print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "  print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "  # Save the model if the current loss is the best so far\n",
        "  if avg_train_loss < best_loss:\n",
        "      best_loss = avg_train_loss\n",
        "      name_save = ' '\n",
        "      torch.save(model, name_save)\n",
        "\n",
        "  return loss_values"
      ],
      "metadata": {
        "id": "ZONRNKoBpyM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def macro_score_f1(true_labels,predicted_labels):\n",
        "    best_labels = []\n",
        "    for i,label in enumerate(true_labels):\n",
        "        flag = False\n",
        "        for l in predicted_labels[i]:\n",
        "            if l == label:\n",
        "                best_labels.append(l)\n",
        "                flag = True\n",
        "        if not flag: best_labels.append(predicted_labels[i][0])\n",
        "    #print(len(best_labels))\n",
        "    #print(len(true_labels))\n",
        "    macroF1Score = f1_score(true_labels, best_labels, average='macro')\n",
        "    return macroF1Score\n"
      ],
      "metadata": {
        "id": "GhLVmYovqBeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_score_f1(true_labels,predicted_labels):\n",
        "    best_labels = []\n",
        "    for i,label in enumerate(true_labels):\n",
        "        flag = False\n",
        "        for l in predicted_labels[i]:\n",
        "            if l == label:\n",
        "                best_labels.append(l)\n",
        "                flag = True\n",
        "        if not flag: best_labels.append(predicted_labels[i][0])\n",
        "    #print(len(best_labels))\n",
        "    #print(len(true_labels))\n",
        "    macroF1Score = f1_score(true_labels, best_labels, average='weighted')\n",
        "    return macroF1Score"
      ],
      "metadata": {
        "id": "J3IeWHVYqNFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_fun(validation_dataloader, model):\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "    t0 = time.time()\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # The documentation for this `model` function is here:\n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids,\n",
        "                            token_type_ids=None,\n",
        "                            attention_mask=b_input_mask)\n",
        "\n",
        "        logits = outputs[0]\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "        predictions.append(logits)\n",
        "        true_labels.append(label_ids)\n",
        "    Accs.append(eval_accuracy/nb_eval_steps)\n",
        "    print('    DONE.')\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "    flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "    # Calculate the confusion matrix\n",
        "    conf_matrix = confusion_matrix(flat_true_labels, flat_predictions)\n",
        "\n",
        "    # Plot the confusion matrix\n",
        "    plt.figure(figsize=(25, 20))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.show()\n",
        "\n",
        "    # Print the classification report\n",
        "    classif_rep = classification_report(flat_true_labels, flat_predictions, digits=5)\n",
        "    print(\"Classification Report:\")\n",
        "    print(classif_rep)\n",
        "    ############################################################################################\n",
        "    flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "    flat_predictions = np.argsort(flat_predictions, axis=1)[:, -3:]  # Get the indices of top 3 predictions\n",
        "    flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "    top3_accuracy = np.mean([1 if true_label in pred_labels else 0 for true_label, pred_labels in zip(flat_true_labels, flat_predictions)])\n",
        "    print(\"  Top 3 Accuracy: {0:.2f}\".format(top3_accuracy))\n",
        "    ##############################################################################\n",
        "    #print(flat_predictions)\n",
        "    #print(flat_true_labels)\n",
        "    #x = f1_score(flat_true_labels, flat_predictions, average='macro')\n",
        "    top3_f1 = macro_score_f1(flat_true_labels,flat_predictions)\n",
        "    print(f\"top 3 macro f1 score : {top3_f1}\")\n",
        "    top3_weightedf1 = weighted_score_f1(flat_true_labels,flat_predictions)\n",
        "    print(f\"top 3 weighted f1 score : {top3_weightedf1}\")\n",
        "    f1score.append(top3_f1)\n",
        "    top3acc.append(top3_accuracy)\n",
        "    Top3_weightedf1.append(top3_weightedf1)"
      ],
      "metadata": {
        "id": "664rGHofqS-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Accs = []\n",
        "losses = []\n",
        "f1score = []\n",
        "top3acc = []\n",
        "Top3_weightedf1  = []\n",
        "num_epochs = 15      #change number of epochs if you want\n",
        "for epoch_i in range(0, num_epochs):\n",
        "  train_fun(epoch_i, train_dataloader, model)\n",
        "\n",
        "  print('Data validation>>>')\n",
        "  test_fun(validation_dataloader, model)"
      ],
      "metadata": {
        "id": "X1t68G6qp_TH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the accuracy per epoch\n",
        "# Assuming you have a list called 'accuracy_list'\n",
        "\n",
        "# Create a list of indices (epochs)\n",
        "epochs = list(range(len(Accs)))\n",
        "\n",
        "# Plot the accuracy values against the epochs\n",
        "plt.plot(epochs, Accs, marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy per Epoch')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mOCBFNMQ5__e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the total loss per epoch\n",
        "\n",
        "epochs = list(range(len(losses)))\n",
        "\n",
        "# Plot the accuracy values against the epochs\n",
        "plt.plot(epochs, losses, marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.title('loss per Epoch')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7uOfRSiz5_2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the f1score per epoch\n",
        "\n",
        "epochs = list(range(len(f1score)))\n",
        "\n",
        "# Plot the accuracy values against the epochs\n",
        "plt.plot(epochs, f1score, marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Macro avg top 1 f1 score')\n",
        "plt.title('f1 score per Epoch')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V9Prc5dD5_lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# loading the best model and pushing it to hugging face"
      ],
      "metadata": {
        "id": "DkNOrGE9rQl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load(\"path to your best model. will be in your directory with name Best_Model.pt\")"
      ],
      "metadata": {
        "id": "ZcBGabyVrPi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token $\"put here your token key, generate it from hugging face website\""
      ],
      "metadata": {
        "id": "7nLZ4aOKr7Pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"give a meaningful name here about your model\")\n"
      ],
      "metadata": {
        "id": "5SPjJn63sDZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n"
      ],
      "metadata": {
        "id": "wOrepVMWsEF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.push_to_hub(\"give a meaningful name here about your model\")"
      ],
      "metadata": {
        "id": "91u89eg9sFoi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}